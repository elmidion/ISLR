---
title: "The analysis of Advertising data for ISLR"
author: "*Kiseong Park; Doctor, Data scientist*"
date: "*Monday, April 22-23, 2019*"
output: html_notebook
---

The goal is to understand statistical learning about linear regression.  
The data is from 'Advertising'  

```{r}
Ad <- read.csv("E:/AI/ISLR/Advertising.csv")
str(Ad)
n = 200
p = 3
```

It has 200 observations, index of markets, 3 features and `sales` label.  

In simple linear regression, we assume one feature has correlation with label.  
Therefore there are two coefficients to be estimate.  

How do we get coefficients?  

We have to use MMSE(minimum mean squared error) method.  
The mean squared error is calculated from (Y - Xモ)^2.  
The matrix X is has 1 as variable to intercept.  

Mean squared error has a form of quadratic equation.  
We can differentiate this equation to minimize the MSE.  

As a result, モ = solve(t(X)X)t(X)Y.  
Now I am goint to get モ.  

```{r}
Y <- Ad$sales
Y <- as.matrix(Y)

b0 <- rep(1, 200)
TV.X <- cbind(b0, Ad$TV)
TV.X <- as.matrix(TV.X)
colnames(TV.X) <- c('モ0', 'モ1')

TV.coefficients <- solve(t(TV.X) %*% TV.X) %*% t(TV.X) %*% Y
TV.coefficients
```

This result means sales = 7.03259355 + 0.04753664*TV  
I draw it as graph.  

```{r}
x = seq(0,300)
y = 7.03259355 + 0.04753664*x
plot(Ad$TV, Ad$sales, type = 'n'); points(Ad$TV, Ad$sales); lines(x, y, type = 'l')
```

The coefficients are compared with coefficients from linear regresion function as basic function of R.  

```{r}
Ad <- Ad[,-1]
Ad.lm <- lm(sales ~ TV, data=Ad)

TV.coefficients; Ad.lm$coefficients
```

We can see they are same.  

This process is point estimation for coefficients.  
It can be confirmed by hypothesis test whether there is a correlation between 'TV' and 'sales'.  

The null hypothesis is that モ1 is zero. 
We don't know the standard deviation of population.  
However, We can use t-distribution.  

t = (observation - mean of population)/standard error of sample.  
Standard error of sample is assumed by sqrt(residual sum of squares/(n-2)).  

The interval estimation is モ1 ‐ sd. It is 0.04753664 ‐ 1.670542, (-1623006, 1.1718079)  

```{r}
TV.RSS <- sum((Ad$sales-(0.04753664*Ad$TV+7.03259355))^2)
TV.RSS
TV.SE.b1 <- sqrt((sqrt(RSS/(n-2)))^2/(sum((Ad$TV-mean(Ad$TV))^2)))
TV.SE.b1

TV.t <- (0.04753644 - 0)/TV.SE.b1
TV.t
TV.pvalue = pt(-TV.t, 198) * 2 

TV.b1.df <- data.frame('coeffient' = TV.coefficients[2], 'standard error' = TV.SE.b1, 't-statistics' = TV.t, 'p-value' = TV.pvalue)
TV.b1.df
```

About intercept of regression by TV,  

```{r}
TV.coefficients[1]

TV.SE.b0 <- sqrt(((sqrt(TV.RSS/(n-2)))^2)*(1/n + (mean(Ad$TV)^2)/(sum((Ad$TV-mean(Ad$TV))^2))))
TV.SE.b0

TV.t.b0 <- (TV.coefficients[1] - 0)/TV.SE.b0
TV.t.b0

TV.b0.pvalue <- pt(-abs(TV.t.b0), n-2) * 2

TV.b0.df <- data.frame('coeffient' = TV.coefficients[1], 'standard error' = TV.SE.b0, 't-statistics' = TV.t.b0, 'p-value' = TV.b0.pvalue)
TV.b0.df

rbind(TV.b0.df, TV.b1.df)
```


Apply this method to radio,

```{r}
Rd.X <- Ad$radio
Rd.X <- cbind(b0, Rd.X)

Rd.coefficients <- solve(t(Rd.X) %*% Rd.X) %*% t(Rd.X) %*% Y
rownames(Rd.coefficients) <- c('モ0', 'モ1')
Rd.coefficients

Rd.RSS <- sum((Ad$sales - (9.3116381 + 0.2024958*Ad$radio))^2)
Rd.SE.b1 <- sqrt((sqrt(Rd.RSS/(n-2)))^2/(sum((Ad$radio - mean(Ad$radio))^2)))
Rd.SE.b1

Rd.t.b1 <- (Rd.coefficients[2] - 0)/Rd.SE.b1
Rd.t.b1

Rd.pvalue <- pt(-abs(Rd.t.b1), n-2) * 2

Rd.b1.df <- data.frame('coeffient' = Rd.coefficients[2], 'standard error' = Rd.SE.b1, 't-statistics' = Rd.t.b1, 'p-value' = Rd.pvalue)
Rd.b1.df
```

Apply same method to newspaper,
```{r}
NW.X <- Ad$newspaper
NW.X <- cbind(b0, NW.X)
NW.X

NW.coefficients <- solve(t(NW.X) %*% NW.X) %*% t(NW.X) %*% Y
rownames(NW.coefficients) <- c('モ0', 'モ1')
NW.coefficients

NW.RSS <- sum((Ad$sales - (12.3514071 + 0.0546931 * Ad$newspaper))^2)
NW.SE.b1 <- sqrt((sqrt(NW.RSS/(n-2)))^2/(sum((Ad$newspaper - mean(Ad$newspaper))^2)))
NW.SE.b1

NW.t.b1 <- (NW.coefficients[2] - 0)/NW.SE.b1
NW.t.b1

NW.b1.pvalue <- pt(-abs(NW.t.b1), n-2) * 2

NW.b1.df <- data.frame('coeffient' = NW.coefficients[2], 'standard error' = NW.SE.b1, 't-statistics' = NW.t.b1, 'p-value' = NW.b1.pvalue)
NW.b1.df
```

Now, I will do multiple regression for advertising data.  
First, coefficients of multiple regression will be assumed.  
The regression has 3 independent variables; TV, radio, and newspaper.  
And it also has intercept.  
Therefore, the matrix of input features 'X' has 4 columns.

```{r message=FALSE}
Ad.X <- cbind(b0, Ad$TV, Ad$radio, Ad$newspaper)
colnames(Ad.X) <- c('モ0', 'モ1', 'モ2', 'モ3')

Ad.m.coefficients <- solve(t(Ad.X) %*% Ad.X) %*% t(Ad.X) %*% Y
Ad.m.coefficients

M.RSS <- sum((Ad$sales - (2.938889369 + 0.045764645*Ad$TV + 0.188530017 * Ad$radio -0.001037493 * Ad$newspaper))^2)

SE.m <- diag(sqrt(solve(t(Ad.X) %*% Ad.X) * M.RSS/(n-4)))
SE.m

M.t.b0 <- (Ad.m.coefficients[1] - 0)/SE.m[1]
M.t.b0

M.t.b1 <- (Ad.m.coefficients[2] - 0)/SE.m[2]
M.t.b1

M.t.b2 <- (Ad.m.coefficients[3] - 0)/SE.m[3]
M.t.b2

M.t.b3 <- (Ad.m.coefficients[4] - 0)/SE.m[4]
M.t.b3

M.pvalue.b0 <- pt(-abs(M.t.b0), n-4) * 2
M.pvalue.b0

M.pvalue.b1 <- pt(-abs(M.t.b1), n-4) * 2
M.pvalue.b1

M.pvalue.b2 <- pt(-abs(M.t.b2), n-4) * 2
M.pvalue.b2

M.pvalue.b3 <- pt(-abs(M.t.b3), n-4) * 2
M.pvalue.b3

M.df <- data.frame('coeffient' = Ad.m.coefficients, 'standard error' = SE.m, 't-statistics' = c(M.t.b0, M.t.b1, M.t.b2, M.t.b3),  'p-value' = c(M.pvalue.b0, M.pvalue.b1, M.pvalue.b2, M.pvalue.b3))
M.df
```

How to get the standard errors of multiple regression is here(https://support.minitab.com/ko-kr/minitab/18/help-and-how-to/modeling-statistics/regression/how-to/fit-regression-model/methods-and-formulas/coefficients/)


How powerful is this multiple regression model?

```{r}
M.TSS <- sum((Ad$sales - mean(Ad$sales))^2)
R2 <- (M.TSS-M.RSS)/M.TSS
R2

M.RSE <- sqrt(M.RSS/(n-4))
M.RSE

F.statistics <- ((M.TSS - M.RSS)/p)/(RSS/(n-p-1))
F.statistics
```

The R-squared of this model is 0.897.  
The rasidual standard error is 1.686.  
F-statistics is 570.


